{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adff4b4b",
   "metadata": {},
   "source": [
    "# YouTube 多说话人字幕系统（Kaggle GPU）\n",
    "\n",
    "> 本 Notebook 为端到端版本，按 13 个阶段组织。\n",
    "> 每个阶段前有 Markdown 说明，便于逐格调试与定位问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec10c9",
   "metadata": {},
   "source": [
    "## 1. 参数区\n",
    "\n",
    "- 定义输入参数与输出路径\n",
    "- 统一缓存与产出到 `/kaggle/working/`\n",
    "- 读取 `HF_TOKEN`（建议来自 Kaggle Secrets）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af456207",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 1] 参数区初始化\")\n",
    "from pathlib import Path\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "YOUTUBE_URL = \"\"\n",
    "DOWNLOAD_MODE = \"single\"  # 可选: \"single\" 或 \"all\"\n",
    "PLAYLIST_INDEX = 1\n",
    "TARGET_REF_AUDIO_PATH = \"/kaggle/working/biao.mp3\"\n",
    "TARGET_REF_AUDIO_URL = \"https://raw.githubusercontent.com/Hana19951208/youtube-speaker-diarization/master/biao.mp3\"\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "ENABLE_DEMUCS = True\n",
    "WHISPER_MODEL = \"large-v3\"\n",
    "LANGUAGE = None\n",
    "NUM_SPEAKERS = None\n",
    "TARGET_SIM_THRESHOLD = 0.3\n",
    "\n",
    "BASE_WORKDIR = Path(\"/kaggle/working\")\n",
    "RAW_AUDIO_DIR = BASE_WORKDIR / \"raw_audio\"\n",
    "PROCESSED_AUDIO_DIR = BASE_WORKDIR / \"processed_audio\"\n",
    "DEMUCS_OUTPUT_DIR = BASE_WORKDIR / \"demucs_output\"\n",
    "VOCALS_DIR = BASE_WORKDIR / \"vocals_audio\"\n",
    "OUTPUT_DIR = BASE_WORKDIR / \"outputs\"\n",
    "MODEL_CACHE_DIR = BASE_WORKDIR / \"model_cache\"\n",
    "TARGET_REF_DIR = BASE_WORKDIR / \"target_ref_processed\"\n",
    "\n",
    "for p in [\n",
    "    RAW_AUDIO_DIR,\n",
    "    PROCESSED_AUDIO_DIR,\n",
    "    DEMUCS_OUTPUT_DIR,\n",
    "    VOCALS_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    MODEL_CACHE_DIR,\n",
    "    TARGET_REF_DIR,\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(MODEL_CACHE_DIR / \"hf_home\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(MODEL_CACHE_DIR / \"transformers\")\n",
    "os.environ[\"TORCH_HOME\"] = str(MODEL_CACHE_DIR / \"torch\")\n",
    "os.environ[\"XDG_CACHE_HOME\"] = str(MODEL_CACHE_DIR / \"xdg\")\n",
    "\n",
    "print(\"参数初始化完成\")\n",
    "print(f\"HF_TOKEN 已设置: {bool(HF_TOKEN)}\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"⚠️ HF_TOKEN 为空：pyannote 将回退为单说话人模式\")\n",
    "print(f\"TARGET_REF_AUDIO_PATH: {TARGET_REF_AUDIO_PATH}\")\n",
    "print(f\"TARGET_REF_AUDIO_URL: {TARGET_REF_AUDIO_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9aa1e1",
   "metadata": {},
   "source": [
    "## 2. GPU 与网络检测\n",
    "\n",
    "- 检查 CUDA 可用性\n",
    "- 检查 Kaggle Internet 开关\n",
    "- 提前提示高风险运行条件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ebdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 2] GPU/网络环境检测\")\n",
    "import socket\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "def check_internet(host=\"huggingface.co\", port=443, timeout=5):\n",
    "    try:\n",
    "        socket.create_connection((host, port), timeout=timeout)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "INTERNET_AVAILABLE = check_internet()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"=== 环境检测 ===\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Torch CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA (torch): {torch.version.cuda}\")\n",
    "\n",
    "if not INTERNET_AVAILABLE:\n",
    "    print(\"⚠️ 检测到 Kaggle Internet 可能未开启；请到 Notebook Settings 打开 Internet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1617972",
   "metadata": {},
   "source": [
    "## 3. 依赖安装（锁版本 + 兼容修复）\n",
    "\n",
    "- **先强制重装科学计算栈**，修复 `numpy/scipy/sklearn` ABI 混装问题\n",
    "- 再安装 ASR/diarization 栈\n",
    "- 优先保留 Kaggle 预置 torch，不强制重装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51150358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 3] 依赖安装（含 Python 3.12 兼容修复）\")\n",
    "import sys\n",
    "import subprocess\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# 关键防护：如果你在同一会话中反复重装 numpy/scipy/sklearn，容易出现 ABI 混态\n",
    "preloaded = [m for m in (\"numpy\", \"scipy\", \"sklearn\") if m in sys.modules]\n",
    "if preloaded:\n",
    "    raise RuntimeError(\n",
    "        f\"检测到已加载模块 {preloaded}。请先 Restart Session，再从 Cell 1 -> Cell 3 顺序重跑。\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_cmd_capture(cmd):\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    return subprocess.run(cmd, text=True, capture_output=True)\n",
    "\n",
    "\n",
    "def pip_install(pkgs, force_reinstall=False, constraints_file=None, no_deps=False):\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"--no-cache-dir\",\n",
    "        \"--upgrade-strategy\",\n",
    "        \"only-if-needed\",\n",
    "    ]\n",
    "    if force_reinstall:\n",
    "        cmd.append(\"--force-reinstall\")\n",
    "    if constraints_file:\n",
    "        cmd.extend([\"-c\", str(constraints_file)])\n",
    "    if no_deps:\n",
    "        cmd.append(\"--no-deps\")\n",
    "    cmd.extend(pkgs)\n",
    "    res = run_cmd_capture(cmd)\n",
    "    if res.returncode != 0:\n",
    "        print(\"stderr tail:\", res.stderr[-8000:])\n",
    "        raise RuntimeError(\"pip install failed\")\n",
    "    print(res.stdout[-1800:])\n",
    "\n",
    "\n",
    "def soft_pip_check():\n",
    "    res = run_cmd_capture([sys.executable, \"-m\", \"pip\", \"check\"])\n",
    "    if res.returncode != 0:\n",
    "        print(\"⚠️ pip check 有冲突（Kaggle/基础镜像预装包常见），仅告警不失败。\")\n",
    "        print((res.stdout or \"\")[-3000:])\n",
    "        print((res.stderr or \"\")[-3000:])\n",
    "    else:\n",
    "        print(\"✅ pip check 通过\")\n",
    "\n",
    "\n",
    "torch_before = None\n",
    "torchaudio_before = None\n",
    "try:\n",
    "    import torch\n",
    "    torch_before = torch.__version__\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import torchaudio\n",
    "    torchaudio_before = torchaudio.__version__\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 约束文件：锁住关键二进制栈，防止 resolver 升级到不兼容版本\n",
    "constraints_path = Path('/kaggle/working/constraints_whisperx_py312.txt')\n",
    "constraints_text = \"\"\"numpy==2.1.3\n",
    "scipy==1.14.1\n",
    "scikit-learn==1.5.2\n",
    "transformers==4.46.3\n",
    "tokenizers==0.20.3\n",
    "accelerate==0.34.2\n",
    "huggingface-hub==0.36.2\n",
    "faster-whisper==1.0.0\n",
    "ctranslate2==4.4.0\n",
    "\"\"\"\n",
    "constraints_path.write_text(constraints_text, encoding='utf-8')\n",
    "print(f\"写入 constraints: {constraints_path}\")\n",
    "\n",
    "# 先统一科学栈（NumPy 2.1.x，避免 NumPy 2.4.x 链路问题）\n",
    "numeric_stack = [\"numpy==2.1.3\", \"scipy==1.14.1\", \"scikit-learn==1.5.2\"]\n",
    "pip_install(numeric_stack, force_reinstall=True)\n",
    "\n",
    "# 方案 A：在 constraints 下正常解析安装\n",
    "plan_a = [\n",
    "    \"yt-dlp==2025.2.19\",\n",
    "    \"ffmpeg-python==0.2.0\",\n",
    "    \"demucs==4.0.1\",\n",
    "    \"faster-whisper==1.0.0\",\n",
    "    \"ctranslate2==4.4.0\",\n",
    "    \"whisperx==3.2.0\",\n",
    "    \"pyannote.audio==3.1.1\",\n",
    "    \"speechbrain==0.5.16\",\n",
    "    \"soundfile==0.12.1\",\n",
    "    \"transformers==4.46.3\",\n",
    "    \"accelerate==0.34.2\",\n",
    "]\n",
    "\n",
    "# 方案 B：先装依赖，再 no-deps 装 whisperx\n",
    "plan_b_deps = [\n",
    "    \"yt-dlp==2025.2.19\",\n",
    "    \"ffmpeg-python==0.2.0\",\n",
    "    \"demucs==4.0.1\",\n",
    "    \"faster-whisper==1.0.0\",\n",
    "    \"ctranslate2==4.4.0\",\n",
    "    \"pyannote.audio==3.1.1\",\n",
    "    \"speechbrain==0.5.16\",\n",
    "    \"soundfile==0.12.1\",\n",
    "    \"transformers==4.46.3\",\n",
    "    \"accelerate==0.34.2\",\n",
    "]\n",
    "\n",
    "installed = False\n",
    "\n",
    "try:\n",
    "    print(\"\\n=== 尝试方案 A（constraints + 正常解析）===\")\n",
    "    pip_install(plan_a, constraints_file=constraints_path)\n",
    "\n",
    "    import numpy  # noqa: F401\n",
    "    import scipy  # noqa: F401\n",
    "    import sklearn  # noqa: F401\n",
    "    import transformers  # noqa: F401\n",
    "    import whisperx  # noqa: F401\n",
    "    import pyannote.audio  # noqa: F401\n",
    "\n",
    "    print(\"✅ 方案 A 安装并导入验证成功\")\n",
    "    installed = True\n",
    "except Exception as e:\n",
    "    print(f\"❌ 方案 A 失败: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "if not installed:\n",
    "    try:\n",
    "        print(\"\\n=== 尝试方案 B（constraints + whisperx no-deps）===\")\n",
    "        pip_install(plan_b_deps, constraints_file=constraints_path)\n",
    "        pip_install([\"whisperx==3.2.0\"], constraints_file=constraints_path, no_deps=True)\n",
    "\n",
    "        import numpy  # noqa: F401\n",
    "        import scipy  # noqa: F401\n",
    "        import sklearn  # noqa: F401\n",
    "        import transformers  # noqa: F401\n",
    "        import whisperx  # noqa: F401\n",
    "        import pyannote.audio  # noqa: F401\n",
    "\n",
    "        print(\"✅ 方案 B 安装并导入验证成功\")\n",
    "        installed = True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 方案 B 失败: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if not installed:\n",
    "    raise RuntimeError(\"所有依赖方案都失败，请检查 Kaggle Internet 与 pip 日志\")\n",
    "\n",
    "soft_pip_check()\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch(before): {torch_before}, torch(after): {torch.__version__}\")\n",
    "except Exception:\n",
    "    print(\"⚠️ torch 导入失败\")\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    print(f\"torchaudio(before): {torchaudio_before}, torchaudio(after): {torchaudio.__version__}\")\n",
    "except Exception:\n",
    "    print(\"⚠️ torchaudio 导入失败\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b913eaa",
   "metadata": {},
   "source": [
    "## 4. 依赖验证（含 whisperx smoke test）\n",
    "\n",
    "- 打印关键库版本\n",
    "- 检查 ffmpeg\n",
    "- 立即验证 `import whisperx`，尽早失败、尽早定位\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 4] 依赖验证与 whisperx smoke test\")\n",
    "import importlib\n",
    "import importlib.metadata as md\n",
    "import subprocess\n",
    "\n",
    "def pkg_ver(name):\n",
    "    try:\n",
    "        return md.version(name)\n",
    "    except Exception:\n",
    "        return \"NOT_FOUND\"\n",
    "\n",
    "for name in [\n",
    "    \"torch\", \"torchaudio\", \"numpy\", \"scipy\", \"scikit-learn\",\n",
    "    \"transformers\", \"yt-dlp\", \"demucs\", \"whisperx\", \"pyannote.audio\", \"speechbrain\", \"soundfile\"\n",
    "]:\n",
    "    print(f\"{name}: {pkg_ver(name)}\")\n",
    "\n",
    "ffmpeg_check = subprocess.run([\"ffmpeg\", \"-version\"], text=True, capture_output=True)\n",
    "if ffmpeg_check.returncode == 0:\n",
    "    print((ffmpeg_check.stdout.splitlines() or [\"ffmpeg ok\"])[0])\n",
    "else:\n",
    "    raise RuntimeError(\"ffmpeg 不可用\")\n",
    "\n",
    "# 关键：提前 smoke test，避免在后续功能 cell 才爆栈\n",
    "try:\n",
    "    import numpy  # noqa: F401\n",
    "    import scipy  # noqa: F401\n",
    "    import sklearn  # noqa: F401\n",
    "    import transformers  # noqa: F401\n",
    "    import whisperx  # noqa: F401\n",
    "    print(\"✅ whisperx 及依赖导入成功\")\n",
    "except Exception as e:\n",
    "    print(\"❌ whisperx 导入失败，错误如下：\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5356e",
   "metadata": {},
   "source": [
    "## 5. 下载模块\n",
    "\n",
    "- 用 `yt-dlp` 下载 YouTube 音频\n",
    "- 自动识别单视频 / playlist\n",
    "- 支持 `single` 指定集数与 `all` 全量模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcb28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 5] 定义下载模块\")\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import yt_dlp\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    name = re.sub(r\"[^\\w\\-.]+\", \"_\", name.strip(), flags=re.UNICODE)\n",
    "    return name[:120] if len(name) > 120 else name\n",
    "\n",
    "\n",
    "def detect_playlist(url: str) -> bool:\n",
    "    probe_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"extract_flat\": \"in_playlist\",\n",
    "        \"noplaylist\": False,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(probe_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "    return bool(info.get(\"_type\") == \"playlist\" or \"entries\" in info)\n",
    "\n",
    "\n",
    "def download_youtube_audio(url: str, mode: str = \"single\", playlist_index: int = 1) -> List[str]:\n",
    "    if not url.strip():\n",
    "        raise ValueError(\"YOUTUBE_URL 为空，请先填写\")\n",
    "\n",
    "    mode = mode.lower().strip()\n",
    "    if mode not in {\"single\", \"all\"}:\n",
    "        raise ValueError(\"DOWNLOAD_MODE 只支持 'single' 或 'all'\")\n",
    "    if playlist_index < 1:\n",
    "        raise ValueError(\"PLAYLIST_INDEX 必须 >= 1\")\n",
    "\n",
    "    print(\"=== YouTube 下载开始 ===\")\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"MODE: {mode}, PLAYLIST_INDEX: {playlist_index}\")\n",
    "\n",
    "    is_playlist = detect_playlist(url)\n",
    "    print(f\"检测类型: {'Playlist/合集' if is_playlist else '单视频'}\")\n",
    "\n",
    "    before_files = set(RAW_AUDIO_DIR.glob(\"*.wav\"))\n",
    "    start_ts = time.time()\n",
    "\n",
    "    def progress_hook(d):\n",
    "        status = d.get(\"status\", \"\")\n",
    "        if status == \"downloading\":\n",
    "            pct = d.get(\"_percent_str\", \"\").strip()\n",
    "            spd = d.get(\"_speed_str\", \"\").strip()\n",
    "            eta = d.get(\"_eta_str\", \"\").strip()\n",
    "            fname = os.path.basename(d.get(\"filename\", \"unknown\"))\n",
    "            print(f\"\\r下载中: {fname} | {pct} | {spd} | ETA {eta}\", end=\"\")\n",
    "        elif status == \"finished\":\n",
    "            fname = os.path.basename(d.get(\"filename\", \"unknown\"))\n",
    "            print(f\"\\n✅ 下载完成(待转码): {fname}\")\n",
    "\n",
    "    outtmpl = (\n",
    "        str(RAW_AUDIO_DIR / \"%(playlist_index)s_%(id)s_%(title).120B.%(ext)s\")\n",
    "        if is_playlist\n",
    "        else str(RAW_AUDIO_DIR / \"%(id)s_%(title).120B.%(ext)s\")\n",
    "    )\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        \"outtmpl\": outtmpl,\n",
    "        \"noplaylist\": (mode == \"single\" and not is_playlist),\n",
    "        \"playlist_items\": str(playlist_index) if (mode == \"single\" and is_playlist) else None,\n",
    "        \"ignoreerrors\": False,\n",
    "        \"continuedl\": True,\n",
    "        \"restrictfilenames\": True,\n",
    "        \"progress_hooks\": [progress_hook],\n",
    "        \"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\"}],\n",
    "        \"quiet\": False,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    after_files = set(RAW_AUDIO_DIR.glob(\"*.wav\"))\n",
    "    new_files = sorted(after_files - before_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "    if not new_files:\n",
    "        new_files = sorted(\n",
    "            [p for p in RAW_AUDIO_DIR.glob(\"*.wav\") if p.stat().st_mtime >= start_ts - 3],\n",
    "            key=lambda p: p.stat().st_mtime,\n",
    "        )\n",
    "\n",
    "    if mode == \"single\":\n",
    "        if not new_files:\n",
    "            raise RuntimeError(\"single 模式未找到下载结果\")\n",
    "        selected = [str(new_files[-1])]\n",
    "    else:\n",
    "        selected = [str(p) for p in new_files]\n",
    "\n",
    "    if not selected:\n",
    "        raise RuntimeError(\"未找到下载后的 wav 文件\")\n",
    "\n",
    "    print(f\"\\n下载完成，共 {len(selected)} 个音频\")\n",
    "    for p in selected[:10]:\n",
    "        print(\" -\", p)\n",
    "    if len(selected) > 10:\n",
    "        print(f\" ...其余 {len(selected) - 10} 个已省略\")\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f5c93",
   "metadata": {},
   "source": [
    "## 6. 预处理模块\n",
    "\n",
    "- 使用 ffmpeg 统一音频格式\n",
    "- 输出 `16kHz + mono + PCM s16le`\n",
    "- 输出路径固定到 `/kaggle/working/processed_audio/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 6] 定义预处理模块\")\n",
    "import subprocess\n",
    "\n",
    "def run_cmd(cmd, check=True):\n",
    "    res = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    if check and res.returncode != 0:\n",
    "        print(\"命令失败:\", \" \".join(cmd))\n",
    "        print(\"stdout:\", res.stdout[-3000:])\n",
    "        print(\"stderr:\", res.stderr[-3000:])\n",
    "        raise RuntimeError(f\"Command failed: {' '.join(cmd)}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def preprocess_audio_ffmpeg(input_audio_path: str, output_dir: Path = PROCESSED_AUDIO_DIR) -> str:\n",
    "    input_path = Path(input_audio_path)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"输入音频不存在: {input_audio_path}\")\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stem = sanitize_filename(input_path.stem)\n",
    "    output_path = output_dir / f\"{stem}_16k_mono.wav\"\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(input_path),\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-c:a\", \"pcm_s16le\",\n",
    "        str(output_path),\n",
    "    ]\n",
    "    run_cmd(cmd, check=True)\n",
    "    return str(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44412d",
   "metadata": {},
   "source": [
    "## 7. Demucs 模块（可开关）\n",
    "\n",
    "- `ENABLE_DEMUCS=True` 时执行 `htdemucs --two-stems=vocals`\n",
    "- 若失败自动 fallback 到原音频\n",
    "- 固定输出到 `/kaggle/working/vocals_audio/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 7] 定义 Demucs 模块\")\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def separate_vocals_demucs(input_audio_path: str, enable_demucs: bool = True) -> str:\n",
    "    if not enable_demucs:\n",
    "        print(\"Demucs 已关闭，跳过\")\n",
    "        return input_audio_path\n",
    "\n",
    "    input_path = Path(input_audio_path)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"输入音频不存在: {input_audio_path}\")\n",
    "\n",
    "    print(f\"=== Demucs 分离: {input_audio_path} ===\")\n",
    "    start_ts = time.time()\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"demucs.separate\",\n",
    "        \"-n\", \"htdemucs\",\n",
    "        \"--two-stems=vocals\",\n",
    "        \"-o\", str(DEMUCS_OUTPUT_DIR),\n",
    "        str(input_path),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        run_cmd(cmd, check=True)\n",
    "        candidates = sorted(DEMUCS_OUTPUT_DIR.glob(\"**/vocals.wav\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not candidates:\n",
    "            raise RuntimeError(\"未找到 vocals.wav\")\n",
    "\n",
    "        selected = None\n",
    "        for c in candidates:\n",
    "            if c.stat().st_mtime >= start_ts - 5:\n",
    "                selected = c\n",
    "                break\n",
    "        if selected is None:\n",
    "            selected = candidates[0]\n",
    "\n",
    "        out_path = VOCALS_DIR / f\"{sanitize_filename(input_path.stem)}_vocals.wav\"\n",
    "        shutil.copy2(selected, out_path)\n",
    "        print(f\"✅ Demucs 成功: {out_path}\")\n",
    "        return str(out_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Demucs 失败，fallback 原音频: {e}\")\n",
    "        return input_audio_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d084a58",
   "metadata": {},
   "source": [
    "## 8. WhisperX 转写模块\n",
    "\n",
    "- GPU 优先，自动降级\n",
    "- 自动选择 compute_type\n",
    "- 执行 alignment，失败时回退到未对齐段落\n",
    "- 输出句子级 segment 结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 8] 定义 WhisperX 模块\")\n",
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "\n",
    "\n",
    "def _normalize_whisper_segments(segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for s in segments:\n",
    "        text = (s.get(\"text\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        start = float(s.get(\"start\", 0.0))\n",
    "        end = float(s.get(\"end\", start))\n",
    "        if end < start:\n",
    "            end = start\n",
    "        out.append({\"start\": start, \"end\": end, \"text\": text})\n",
    "    return out\n",
    "\n",
    "\n",
    "def transcribe_with_whisperx(\n",
    "    audio_path: str,\n",
    "    model_name: str = \"large-v3\",\n",
    "    language: Optional[str] = None,\n",
    ") -> Tuple[List[Dict[str, Any]], str]:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 16 if device == \"cuda\" else 4\n",
    "    compute_candidates = [\"float16\", \"int8\"] if device == \"cuda\" else [\"int8\"]\n",
    "\n",
    "    model = None\n",
    "    used_compute = None\n",
    "    for ct in compute_candidates:\n",
    "        try:\n",
    "            print(f\"加载 WhisperX: model={model_name}, device={device}, compute_type={ct}\")\n",
    "            model = whisperx.load_model(\n",
    "                model_name,\n",
    "                device=device,\n",
    "                compute_type=ct,\n",
    "                download_root=str(MODEL_CACHE_DIR / \"whisperx\"),\n",
    "            )\n",
    "            used_compute = ct\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"加载失败 compute_type={ct}: {e}\")\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"WhisperX 模型加载失败\")\n",
    "\n",
    "    print(f\"WhisperX 已加载，compute_type={used_compute}\")\n",
    "    result = model.transcribe(audio_path, batch_size=batch_size, language=language)\n",
    "    detected_lang = result.get(\"language\", language if language else \"unknown\")\n",
    "    segments = _normalize_whisper_segments(result.get(\"segments\", []))\n",
    "\n",
    "    try:\n",
    "        print(\"执行 alignment...\")\n",
    "        align_model, metadata = whisperx.load_align_model(\n",
    "            language_code=detected_lang,\n",
    "            device=device,\n",
    "            model_dir=str(MODEL_CACHE_DIR / \"whisperx_align\"),\n",
    "        )\n",
    "        aligned = whisperx.align(\n",
    "            result[\"segments\"],\n",
    "            align_model,\n",
    "            metadata,\n",
    "            audio_path,\n",
    "            device,\n",
    "            return_char_alignments=False,\n",
    "        )\n",
    "        aligned_segments = _normalize_whisper_segments(aligned.get(\"segments\", []))\n",
    "        if aligned_segments:\n",
    "            segments = aligned_segments\n",
    "            print(\"✅ alignment 成功\")\n",
    "        else:\n",
    "            print(\"⚠️ alignment 为空，使用原始段落\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ alignment 失败，fallback 原始段落: {e}\")\n",
    "\n",
    "    try:\n",
    "        del model\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return segments, detected_lang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075169c",
   "metadata": {},
   "source": [
    "## 9. pyannote 说话人分离模块\n",
    "\n",
    "- 使用 `pyannote/speaker-diarization-3.1`\n",
    "- 支持 `NUM_SPEAKERS` 指定\n",
    "- 若失败，回退为单 speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 9] 定义 Diarization 模块\")\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def get_audio_duration_sec(audio_path: str) -> float:\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-show_entries\", \"format=duration\",\n",
    "        \"-of\", \"default=nokey=1:noprint_wrappers=1\",\n",
    "        audio_path,\n",
    "    ]\n",
    "    res = run_cmd(cmd, check=False)\n",
    "    if res.returncode != 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(res.stdout.strip())\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def fallback_single_speaker_turns(audio_path: str) -> List[Dict[str, Any]]:\n",
    "    dur = get_audio_duration_sec(audio_path)\n",
    "    return [{\"start\": 0.0, \"end\": max(dur, 0.01), \"speaker\": \"SPEAKER_00\"}]\n",
    "\n",
    "\n",
    "def diarize_with_pyannote(\n",
    "    audio_path: str,\n",
    "    hf_token: str,\n",
    "    num_speakers: Optional[int] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    if not hf_token.strip():\n",
    "        print(\"⚠️ HF_TOKEN 为空，回退单说话人\")\n",
    "        return fallback_single_speaker_turns(audio_path)\n",
    "\n",
    "    try:\n",
    "        from pyannote.audio import Pipeline\n",
    "        import torch as _torch\n",
    "\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=hf_token,\n",
    "            cache_dir=str(MODEL_CACHE_DIR / \"pyannote\"),\n",
    "        )\n",
    "\n",
    "        if _torch.cuda.is_available():\n",
    "            pipeline.to(_torch.device(\"cuda\"))\n",
    "\n",
    "        kwargs = {}\n",
    "        if num_speakers is not None:\n",
    "            kwargs[\"num_speakers\"] = int(num_speakers)\n",
    "\n",
    "        diarization = pipeline(audio_path, **kwargs)\n",
    "\n",
    "        turns = []\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            turns.append({\n",
    "                \"start\": float(turn.start),\n",
    "                \"end\": float(turn.end),\n",
    "                \"speaker\": str(speaker),\n",
    "            })\n",
    "\n",
    "        turns.sort(key=lambda x: x[\"start\"])\n",
    "        if not turns:\n",
    "            print(\"⚠️ pyannote 返回空，回退单说话人\")\n",
    "            return fallback_single_speaker_turns(audio_path)\n",
    "\n",
    "        print(f\"✅ diarization 完成，turn 数: {len(turns)}\")\n",
    "        return turns\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ diarization 失败，回退单说话人: {e}\")\n",
    "        return fallback_single_speaker_turns(audio_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97907a",
   "metadata": {},
   "source": [
    "## 10. Target 识别模块\n",
    "\n",
    "- 自动确保参考音频可用（本地不存在则从 GitHub 下载）\n",
    "- 每个 speaker 累计 30~90 秒采样计算 embedding\n",
    "- 输出 `target_id / speaker_scores / confidence_flag`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22433194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 10] 定义 Target 识别模块\")\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "def ensure_target_reference_audio(local_path: str, source_url: str) -> str:\n",
    "    lp = Path(local_path)\n",
    "    if lp.exists():\n",
    "        return str(lp)\n",
    "    if not source_url.strip():\n",
    "        return str(lp)\n",
    "\n",
    "    print(f\"参考音频不存在，尝试下载: {source_url}\")\n",
    "    lp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\"curl\", \"-L\", \"--fail\", source_url, \"-o\", str(lp)]\n",
    "    res = run_cmd(cmd, check=False)\n",
    "    if res.returncode != 0:\n",
    "        print(\"参考音频下载失败 stderr:\")\n",
    "        print(res.stderr[-2000:])\n",
    "        return str(lp)\n",
    "\n",
    "    print(f\"✅ 参考音频已下载: {lp}\")\n",
    "    return str(lp)\n",
    "\n",
    "\n",
    "def load_wave_slice(audio_path: str, start_sec: float, end_sec: float, target_sr: int = 16000):\n",
    "    if end_sec <= start_sec:\n",
    "        return None\n",
    "\n",
    "    info = torchaudio.info(audio_path)\n",
    "    src_sr = info.sample_rate\n",
    "\n",
    "    frame_offset = max(0, int(start_sec * src_sr))\n",
    "    num_frames = max(1, int((end_sec - start_sec) * src_sr))\n",
    "\n",
    "    wav, sr = torchaudio.load(audio_path, frame_offset=frame_offset, num_frames=num_frames)\n",
    "    if wav.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != target_sr:\n",
    "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
    "\n",
    "    return wav\n",
    "\n",
    "\n",
    "def compute_speaker_embedding(classifier, waveform: torch.Tensor, device: str = \"cpu\") -> torch.Tensor:\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    elif waveform.dim() == 2 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = classifier.encode_batch(waveform.to(device)).squeeze().detach().cpu()\n",
    "    return emb\n",
    "\n",
    "\n",
    "def identify_target_speaker(\n",
    "    audio_path: str,\n",
    "    turns: List[Dict[str, Any]],\n",
    "    target_ref_audio_path: str,\n",
    "    threshold: float = 0.3,\n",
    ") -> Tuple[Optional[str], Dict[str, float], bool]:\n",
    "    speaker_scores: Dict[str, float] = {}\n",
    "\n",
    "    if not target_ref_audio_path.strip():\n",
    "        print(\"未提供 TARGET_REF_AUDIO_PATH，跳过 target 识别\")\n",
    "        return None, speaker_scores, False\n",
    "\n",
    "    ref_path = Path(target_ref_audio_path)\n",
    "    if not ref_path.exists():\n",
    "        print(f\"⚠️ 参考音频不存在: {target_ref_audio_path}\")\n",
    "        return None, speaker_scores, False\n",
    "\n",
    "    if not turns:\n",
    "        print(\"⚠️ turns 为空，跳过 target 识别\")\n",
    "        return None, speaker_scores, False\n",
    "\n",
    "    try:\n",
    "        from speechbrain.inference.speaker import EncoderClassifier\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ speechbrain 导入失败: {e}\")\n",
    "        return None, speaker_scores, False\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ref_processed = preprocess_audio_ffmpeg(str(ref_path), TARGET_REF_DIR)\n",
    "    ref_wav, ref_sr = torchaudio.load(ref_processed)\n",
    "    if ref_wav.shape[0] > 1:\n",
    "        ref_wav = ref_wav.mean(dim=0, keepdim=True)\n",
    "    if ref_sr != 16000:\n",
    "        ref_wav = torchaudio.functional.resample(ref_wav, ref_sr, 16000)\n",
    "\n",
    "    ref_sec = ref_wav.shape[1] / 16000.0\n",
    "    if ref_sec < 3.0:\n",
    "        print(f\"⚠️ 参考音频过短: {ref_sec:.2f}s（建议 >= 3s）\")\n",
    "\n",
    "    classifier = EncoderClassifier.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "        savedir=str(MODEL_CACHE_DIR / \"speechbrain\"),\n",
    "        run_opts={\"device\": device},\n",
    "    )\n",
    "\n",
    "    ref_emb = compute_speaker_embedding(classifier, ref_wav, device=device)\n",
    "\n",
    "    speaker_turns = defaultdict(list)\n",
    "    for t in sorted(turns, key=lambda x: x[\"start\"]):\n",
    "        speaker_turns[t[\"speaker\"]].append(t)\n",
    "\n",
    "    for spk, tlist in speaker_turns.items():\n",
    "        chunks = []\n",
    "        total_sec = 0.0\n",
    "        for t in tlist:\n",
    "            if total_sec >= 90.0:\n",
    "                break\n",
    "            s, e = float(t[\"start\"]), float(t[\"end\"])\n",
    "            if e <= s:\n",
    "                continue\n",
    "\n",
    "            remain = 90.0 - total_sec\n",
    "            e = min(e, s + remain)\n",
    "\n",
    "            w = load_wave_slice(audio_path, s, e, target_sr=16000)\n",
    "            if w is None or w.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            chunks.append(w)\n",
    "            total_sec += w.shape[1] / 16000.0\n",
    "\n",
    "        print(f\"[采样时长] {spk}: {total_sec:.2f}s\")\n",
    "        if total_sec <= 0.2:\n",
    "            continue\n",
    "        if total_sec < 30.0:\n",
    "            print(f\"⚠️ {spk} 采样不足 30s，结果可能不稳定\")\n",
    "\n",
    "        spk_wav = torch.cat(chunks, dim=1)\n",
    "        spk_emb = compute_speaker_embedding(classifier, spk_wav, device=device)\n",
    "        sim = float(F.cosine_similarity(ref_emb.unsqueeze(0), spk_emb.unsqueeze(0)).item())\n",
    "        speaker_scores[spk] = sim\n",
    "        print(f\"[相似度] {spk}: {sim:.4f}\")\n",
    "\n",
    "    if not speaker_scores:\n",
    "        print(\"⚠️ 未得到有效 speaker embedding\")\n",
    "        return None, speaker_scores, False\n",
    "\n",
    "    target_id = max(speaker_scores, key=speaker_scores.get)\n",
    "    best_score = speaker_scores[target_id]\n",
    "    confidence_flag = best_score >= threshold\n",
    "\n",
    "    print(f\"[最终选择] target_id={target_id}, score={best_score:.4f}, threshold={threshold}\")\n",
    "    if not confidence_flag:\n",
    "        print(\"⚠️ 低于阈值，标记为不确定\")\n",
    "\n",
    "    return target_id, speaker_scores, confidence_flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee48032",
   "metadata": {},
   "source": [
    "## 11. 对齐与输出模块\n",
    "\n",
    "- segment 与 diarization turn 按重叠时间对齐\n",
    "- speaker 映射为 `speaker1/speaker2...`\n",
    "- target speaker 显示为 `TARGET`\n",
    "- 输出 `.srt` 与 `.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 11] 定义对齐与输出模块\")\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def overlap_duration(a_start: float, a_end: float, b_start: float, b_end: float) -> float:\n",
    "    return max(0.0, min(a_end, b_end) - max(a_start, b_start))\n",
    "\n",
    "\n",
    "def pick_speaker_for_segment(seg, turns):\n",
    "    s0, s1 = float(seg[\"start\"]), float(seg[\"end\"])\n",
    "    best_spk = None\n",
    "    best_ov = -1.0\n",
    "\n",
    "    for t in turns:\n",
    "        ov = overlap_duration(s0, s1, float(t[\"start\"]), float(t[\"end\"]))\n",
    "        if ov > best_ov:\n",
    "            best_ov = ov\n",
    "            best_spk = t[\"speaker\"]\n",
    "\n",
    "    if best_spk is not None and best_ov > 0:\n",
    "        return best_spk\n",
    "\n",
    "    seg_mid = (s0 + s1) / 2.0\n",
    "    nearest_spk = turns[0][\"speaker\"] if turns else \"SPEAKER_00\"\n",
    "    min_dist = float(\"inf\")\n",
    "    for t in turns:\n",
    "        mid = (float(t[\"start\"]) + float(t[\"end\"])) / 2.0\n",
    "        d = abs(mid - seg_mid)\n",
    "        if d < min_dist:\n",
    "            min_dist = d\n",
    "            nearest_spk = t[\"speaker\"]\n",
    "    return nearest_spk\n",
    "\n",
    "\n",
    "def align_segments_with_diarization(segments, turns, target_id, speaker_scores):\n",
    "    turns = sorted(turns, key=lambda x: x[\"start\"]) if turns else [{\"start\": 0.0, \"end\": 1e9, \"speaker\": \"SPEAKER_00\"}]\n",
    "\n",
    "    alias_map = OrderedDict()\n",
    "    next_id = 1\n",
    "    aligned = []\n",
    "\n",
    "    for seg in sorted(segments, key=lambda x: x[\"start\"]):\n",
    "        raw_spk = pick_speaker_for_segment(seg, turns)\n",
    "\n",
    "        if raw_spk not in alias_map:\n",
    "            alias_map[raw_spk] = f\"speaker{next_id}\"\n",
    "            next_id += 1\n",
    "\n",
    "        is_target = (target_id is not None and raw_spk == target_id)\n",
    "        display_spk = \"TARGET\" if is_target else alias_map[raw_spk]\n",
    "        sim_score = float(speaker_scores[raw_spk]) if raw_spk in speaker_scores else None\n",
    "\n",
    "        aligned.append({\n",
    "            \"start\": float(seg[\"start\"]),\n",
    "            \"end\": float(seg[\"end\"]),\n",
    "            \"text\": (seg[\"text\"] or \"\").strip(),\n",
    "            \"speaker\": display_spk,\n",
    "            \"is_target\": bool(is_target),\n",
    "            \"similarity_score\": sim_score,\n",
    "        })\n",
    "\n",
    "    return aligned\n",
    "\n",
    "\n",
    "def sec_to_srt_time(sec: float) -> str:\n",
    "    ms = int(round(sec * 1000))\n",
    "    h = ms // 3600000\n",
    "    ms %= 3600000\n",
    "    m = ms // 60000\n",
    "    ms %= 60000\n",
    "    s = ms // 1000\n",
    "    ms %= 1000\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "\n",
    "\n",
    "def save_srt_json(aligned_segments, audio_path):\n",
    "    base = sanitize_filename(Path(audio_path).stem)\n",
    "    srt_path = OUTPUT_DIR / f\"output_{base}.srt\"\n",
    "    json_path = OUTPUT_DIR / f\"output_{base}.json\"\n",
    "\n",
    "    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, seg in enumerate(aligned_segments, 1):\n",
    "            f.write(\n",
    "                f\"{i}\\n\"\n",
    "                f\"{sec_to_srt_time(seg['start'])} --> {sec_to_srt_time(seg['end'])}\\n\"\n",
    "                f\"{seg['speaker']}: {seg['text']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "    json_data = [{\n",
    "        \"start\": seg[\"start\"],\n",
    "        \"end\": seg[\"end\"],\n",
    "        \"text\": seg[\"text\"],\n",
    "        \"speaker\": seg[\"speaker\"],\n",
    "        \"is_target\": seg[\"is_target\"],\n",
    "        \"similarity_score\": seg[\"similarity_score\"],\n",
    "    } for seg in aligned_segments]\n",
    "\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ 输出: {srt_path}\")\n",
    "    print(f\"✅ 输出: {json_path}\")\n",
    "    return str(srt_path), str(json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710e877",
   "metadata": {},
   "source": [
    "## 12. 主流程运行\n",
    "\n",
    "- 逐视频顺序处理（支持 `single/all`）\n",
    "- 每阶段计时、捕获异常、不中断整体批处理\n",
    "- 汇总每个视频的处理结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53255c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 12] 执行主流程\")\n",
    "import traceback\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def process_one_audio(raw_audio_path: str, idx: int, total: int):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[{idx}/{total}] 开始处理: {raw_audio_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    result = {\n",
    "        \"input_audio\": raw_audio_path,\n",
    "        \"status\": \"success\",\n",
    "        \"error\": None,\n",
    "        \"stage_time_sec\": {},\n",
    "        \"srt_path\": None,\n",
    "        \"json_path\": None,\n",
    "        \"target_id\": None,\n",
    "        \"target_confidence\": False,\n",
    "        \"speaker_scores\": {},\n",
    "        \"speaker_distribution\": {},\n",
    "    }\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        processed_audio = preprocess_audio_ffmpeg(raw_audio_path, PROCESSED_AUDIO_DIR)\n",
    "        result[\"stage_time_sec\"][\"preprocess\"] = round(time.time() - t0, 3)\n",
    "\n",
    "        t0 = time.time()\n",
    "        work_audio = separate_vocals_demucs(processed_audio, enable_demucs=ENABLE_DEMUCS)\n",
    "        result[\"stage_time_sec\"][\"demucs\"] = round(time.time() - t0, 3)\n",
    "\n",
    "        t0 = time.time()\n",
    "        segments, detected_lang = transcribe_with_whisperx(\n",
    "            work_audio,\n",
    "            model_name=WHISPER_MODEL,\n",
    "            language=LANGUAGE,\n",
    "        )\n",
    "        result[\"stage_time_sec\"][\"whisperx\"] = round(time.time() - t0, 3)\n",
    "        result[\"detected_language\"] = detected_lang\n",
    "        print(f\"[whisperx] segments={len(segments)}, language={detected_lang}\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        turns = diarize_with_pyannote(\n",
    "            work_audio,\n",
    "            hf_token=HF_TOKEN,\n",
    "            num_speakers=NUM_SPEAKERS,\n",
    "        )\n",
    "        result[\"stage_time_sec\"][\"diarization\"] = round(time.time() - t0, 3)\n",
    "        print(f\"[diarization] turns={len(turns)}\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        target_id, speaker_scores, confidence_flag = identify_target_speaker(\n",
    "            work_audio,\n",
    "            turns,\n",
    "            TARGET_REF_AUDIO_PATH,\n",
    "            threshold=TARGET_SIM_THRESHOLD,\n",
    "        )\n",
    "        result[\"stage_time_sec\"][\"target_identification\"] = round(time.time() - t0, 3)\n",
    "        result[\"target_id\"] = target_id\n",
    "        result[\"target_confidence\"] = bool(confidence_flag)\n",
    "        result[\"speaker_scores\"] = speaker_scores\n",
    "\n",
    "        t0 = time.time()\n",
    "        aligned_segments = align_segments_with_diarization(\n",
    "            segments,\n",
    "            turns,\n",
    "            target_id=target_id if confidence_flag else None,\n",
    "            speaker_scores=speaker_scores,\n",
    "        )\n",
    "        srt_path, json_path = save_srt_json(aligned_segments, raw_audio_path)\n",
    "        result[\"stage_time_sec\"][\"align_and_export\"] = round(time.time() - t0, 3)\n",
    "        result[\"srt_path\"] = srt_path\n",
    "        result[\"json_path\"] = json_path\n",
    "\n",
    "        dist = Counter([x[\"speaker\"] for x in aligned_segments])\n",
    "        result[\"speaker_distribution\"] = dict(dist)\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"failed\"\n",
    "        result[\"error\"] = str(e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "    result[\"stage_time_sec\"][\"total\"] = round(time.time() - total_start, 3)\n",
    "    print(f\"[完成] status={result['status']}, total={result['stage_time_sec']['total']}s\")\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    if not YOUTUBE_URL.strip():\n",
    "        raise ValueError(\"YOUTUBE_URL 不能为空\")\n",
    "\n",
    "    if not INTERNET_AVAILABLE:\n",
    "        print(\"⚠️ 可能无网络，YouTube 与模型下载可能失败。请开启 Kaggle Internet\")\n",
    "\n",
    "    if not HF_TOKEN.strip():\n",
    "        print(\"⚠️ HF_TOKEN 为空，Diarization 将回退到单 speaker\")\n",
    "\n",
    "    globals()[\"TARGET_REF_AUDIO_PATH\"] = ensure_target_reference_audio(\n",
    "        TARGET_REF_AUDIO_PATH,\n",
    "        TARGET_REF_AUDIO_URL,\n",
    "    )\n",
    "\n",
    "    start_all = time.time()\n",
    "    wav_paths = download_youtube_audio(\n",
    "        YOUTUBE_URL,\n",
    "        mode=DOWNLOAD_MODE,\n",
    "        playlist_index=PLAYLIST_INDEX,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    total = len(wav_paths)\n",
    "    for i, wav_path in enumerate(wav_paths, 1):\n",
    "        results.append(process_one_audio(wav_path, i, total))\n",
    "\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(f\"全部处理完成，视频数={len(results)}，总耗时={round(time.time() - start_all, 2)}s\")\n",
    "    print(\"#\" * 80)\n",
    "    return results\n",
    "\n",
    "\n",
    "RUN_RESULTS = run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c4d01",
   "metadata": {},
   "source": [
    "## 13. 结果统计\n",
    "\n",
    "- 汇总每个视频耗时\n",
    "- 统计 speaker 分布\n",
    "- 打印 target 相似度结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Cell 13] 输出统计结果\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "if \"RUN_RESULTS\" not in globals() or not RUN_RESULTS:\n",
    "    print(\"暂无结果，请先运行主流程\")\n",
    "else:\n",
    "    rows = []\n",
    "    agg_speaker = Counter()\n",
    "\n",
    "    for r in RUN_RESULTS:\n",
    "        agg_speaker.update(r.get(\"speaker_distribution\", {}))\n",
    "\n",
    "        best_score = None\n",
    "        if r.get(\"speaker_scores\"):\n",
    "            best_score = max(r[\"speaker_scores\"].values())\n",
    "\n",
    "        rows.append({\n",
    "            \"input_audio\": r.get(\"input_audio\"),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"total_sec\": r.get(\"stage_time_sec\", {}).get(\"total\"),\n",
    "            \"preprocess_sec\": r.get(\"stage_time_sec\", {}).get(\"preprocess\"),\n",
    "            \"demucs_sec\": r.get(\"stage_time_sec\", {}).get(\"demucs\"),\n",
    "            \"whisperx_sec\": r.get(\"stage_time_sec\", {}).get(\"whisperx\"),\n",
    "            \"diarization_sec\": r.get(\"stage_time_sec\", {}).get(\"diarization\"),\n",
    "            \"target_identification_sec\": r.get(\"stage_time_sec\", {}).get(\"target_identification\"),\n",
    "            \"align_export_sec\": r.get(\"stage_time_sec\", {}).get(\"align_and_export\"),\n",
    "            \"target_id\": r.get(\"target_id\"),\n",
    "            \"target_confidence\": r.get(\"target_confidence\"),\n",
    "            \"best_similarity_score\": best_score,\n",
    "            \"srt_path\": r.get(\"srt_path\"),\n",
    "            \"json_path\": r.get(\"json_path\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df)\n",
    "\n",
    "    print(\"\\n=== Speaker 分布（按字幕段数）===\")\n",
    "    if agg_speaker:\n",
    "        for k, v in agg_speaker.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"无统计数据\")\n",
    "\n",
    "    print(\"\\n=== Target 分数 ===\")\n",
    "    for r in RUN_RESULTS:\n",
    "        name = Path(r.get(\"input_audio\", \"\")).name\n",
    "        scores = r.get(\"speaker_scores\", {})\n",
    "        if scores:\n",
    "            items = \", \".join([f\"{k}={v:.4f}\" for k, v in sorted(scores.items(), key=lambda x: x[1], reverse=True)])\n",
    "            print(f\"{name}: {items}\")\n",
    "        else:\n",
    "            print(f\"{name}: 无 target score\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}